# ソクブツ運用・デプロイガイド

## 🎯 このドキュメントについて

このガイドは、ソクブツシステムの本番環境へのデプロイメントと継続的な運用のための包括的な手順書です。自宅サーバー（WSL2）環境での最適化された運用方法を提供します。

## 🏗️ インフラストラクチャ概要

### 運用環境
- **プラットフォーム**: 自宅サーバー（WSL2 on Windows）
- **コンテナ**: Docker + docker-compose
- **データベース**: SQLite（本番環境）
- **監視**: vibelogger + Telegram通知
- **プロセス管理**: Docker restart policies

### 推奨ハードウェア仕様
- **CPU**: Intel i5-9400T以上（6コア推奨）
- **メモリ**: 8GB以上（運用時2GB、開発時4GB）
- **ストレージ**: SSD 50GB以上の空き容量
- **ネットワーク**: 安定したインターネット接続

## 🚀 WSL2環境セットアップ

### 1. WSL2インストール・設定

#### WSL2有効化
```powershell
# PowerShell（管理者権限）で実行
dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart
dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart

# 再起動後
wsl --set-default-version 2
```

#### Ubuntuインストール
```powershell
# Microsoft StoreからUbuntu 22.04 LTSをインストール
wsl --install -d Ubuntu-22.04

# 確認
wsl --list --verbose
```

#### WSL2パフォーマンス最適化
```bash
# ~/.wslconfig (Windows側)
[wsl2]
memory=4GB
processors=4
swap=2GB
localhostForwarding=true
```

### 2. Ubuntu環境構築

#### システム更新
```bash
sudo apt update && sudo apt upgrade -y
sudo apt install -y build-essential curl git vim
```

#### Node.js インストール
```bash
# Node.js 18.x インストール
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
sudo apt-get install -y nodejs

# バージョン確認
node --version
npm --version
```

#### Dockerインストール
```bash
# Docker公式インストール
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# ユーザーをdockerグループに追加
sudo usermod -aG docker $USER
newgrp docker

# docker-composeインストール
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
```

## 📦 デプロイメント手順

### Webhook公開（Cloudflare Tunnel）

本番はロングポーリングではなく Webhook を使用します。家庭回線でもポート開放不要で常時HTTPS公開できる **Cloudflare Tunnel（無料）** を推奨します。

1) Cloudflareでドメインを管理（Free可）
2) ダッシュボードで Named Tunnel を作成 → 発行された **TUNNEL_TOKEN** を控える
3) Public Hostname を作成
   - 例: `bot.example.com` → Service: `http://localhost:3002`（cloudflaredをホストで動かす場合）
4) `.env` に `ADMIN_PUBLIC_URL=https://bot.example.com` を設定
5) cloudflared を起動（いずれか）
   - 手軽: `cloudflared tunnel run --token <TUNNEL_TOKEN>`
   - 常時運用: systemd もしくは docker-compose で `cloudflared` サービス化
6) アプリを `docker compose up -d` で起動 → 起動時に `/telegram/webhook` をWebhook登録

確認: `curl -s https://api.telegram.org/bot<token>/getWebhookInfo` の `url` が `https://bot.example.com/telegram/webhook`

補足: Cloudflareでゾーン（ドメイン）を管理していない場合は、Public Hostname で安定したカスタムドメインを使えません。以下の「Tailscale Funnel」を利用してください。

### Webhook公開（ゾーン未委譲時の代替：Tailscale Funnel）

Cloudflareでゾーンを管理していない、DNSをすぐに変更できない場合でも、**Tailscale Funnel（無料）**で安定HTTPSの公開URLを取得できます（Let’s Encrypt自動発行）。ポート開放不要・ルーター設定不要です。

手順（WSL2のUbuntuで実行）:

1) Tailscaleインストール
```bash
curl -fsSL https://tailscale.com/install.sh | sh
```

2) ログイン（初回のみ）
```bash
sudo tailscale up
```

3) Webhook公開（管理画面 3002 を外部公開）
```bash
# 3002番で動くAdminServerをHTTPSで公開
tailscale serve --https=443 http://localhost:3002

# Funnelをオン（インターネットへ公開）
tailscale funnel 443 on
```

4) 表示されたURL（例: `https://<host>.<tailnet>.ts.net`）を `.env.production` の `ADMIN_PUBLIC_URL` に設定

5) デプロイ
```bash
docker compose up -d
```

6) 検証
```bash
curl -s https://api.telegram.org/bot$TELEGRAM_BOT_TOKEN/getWebhookInfo | jq
# url が https://<host>.<tailnet>.ts.net/telegram/webhook になっていること
```

運用メモ:
- Tailscale Funnel採用時は、`docker compose up -d sokubutsu-mvp` のようにアプリのみ起動すればOK（`cloudflared`サービスは不要）。
- 証明書はTailscaleが自動管理（Let’s Encrypt）。Webhookシークレット未使用でも運用可能。
- 安定URLは `<host>.<tailnet>.ts.net` となるため、DNSのゾーン移管は不要。

### 本番セットアップ最新版（チェックリスト）

- 環境変数（.env.production）
  - `MULTI_USER_MODE=true`
  - `TELEGRAM_BOT_TOKEN=<本番用>`
  - `TELEGRAM_CHAT_ID=<本番用>`
  - `ADMIN_PUBLIC_URL=<CloudflareまたはTailscaleの公開URL>`
  - `WEBHOOK_GUARDIAN_ENABLED=true`
  - `WEBHOOK_GUARDIAN_INTERVAL=10`（本番は10分推奨）
- 起動
  - `docker compose up -d sokubutsu-mvp`
- 検証
  - Webhook: `curl -s "https://api.telegram.org/bot$TELEGRAM_BOT_TOKEN/getWebhookInfo"`
  - Health: `curl -I "${ADMIN_PUBLIC_URL%/}/health"`
  - Botコマンド: Telegramで `/help`, `/status` が応答
- 運用
  - 自己修復ガードが定期検証→不一致なら自動再設定
  - 早期検証時のみ `WEBHOOK_GUARDIAN_INTERVAL=1` にして短周期で挙動確認（本番戻しを忘れずに）

### 運用用ワンライナー（安全版）

Webhookを期待URLに再設定（コンテナ内の環境変数を用いるためトークンを表示しません）:

```bash
docker exec -i sokubutsu-mvp sh -lc '\
  EXP="${ADMIN_PUBLIC_URL%/}/telegram/webhook"; \
  BASE="https://api.telegram.org/bot${TELEGRAM_BOT_TOKEN}"; \
  curl -fsS -X POST -d "url=$EXP" "$BASE/setWebhook" | jq'
```

現在のWebhook確認:

```bash
curl -fsS "https://api.telegram.org/bot$TELEGRAM_BOT_TOKEN/getWebhookInfo" | jq
```

Funnel/公開URL到達性確認:

```bash
curl -I "${ADMIN_PUBLIC_URL%/}/health"
```

### 1. 本番環境準備

#### リポジトリクローン
```bash
# 本番用ディレクトリ作成
mkdir -p ~/production/sokubutsu
cd ~/production/sokubutsu

# リポジトリクローン
git clone https://github.com/denof-inc/sokubutsu2.git .
git checkout main
```

## 🔁 恒久対策（無応答・手動作業の最小化）

- Webhook自己修復ガード: 既定で有効（`.env.production` で `WEBHOOK_GUARDIAN_ENABLED=true`）。URL不一致は自動復旧。
- Webhook入出力ログ: `admin.webhook.request/response` を常時記録。届いていない/返せていないを即判別。
- 送信失敗リトライ: `sendMessage` は指数バックオフで3回リトライ、最終失敗も落ちずに監視継続。
- 自動デプロイスクリプト（Tailscale含む）:
  - `scripts/ops/deploy-all.sh` … ビルド→起動→Funnel有効化→Webhook設定→検証を一括実行
  - `scripts/ops/ensure-funnel.sh` … Tailscale Serve/Funnel を 3002 に対して有効化し、公開URLを `.env*` に同期
  - `scripts/ops/ensure-webhook.sh` … コンテナ内部の環境変数で Telegram Webhook を設定し `getWebhookInfo` を出力
  - `scripts/ops/verify.sh` … Health / WebhookInfo / Webhook入出力ログをまとめて確認

推奨運用:
- 更新時: `bash scripts/ops/deploy-all.sh`
- 不調時の即時確認: `bash scripts/ops/verify.sh`

（任意）起動時の自動化:
- macOS/Linuxでは systemd/launchd で `ensure-funnel.sh` を起動時に実行すると、再起動後も自動で公開URLを復旧できます。
- 例: systemd unit（/etc/systemd/system/tailscale-funnel.service）
  - `ExecStart=/usr/local/bin/bash /path/to/repo/scripts/ops/ensure-funnel.sh`

### リンク/改行のメッセージ整形ポリシー（運用仕様）
- `parse_mode: 'HTML'`
- 改行は `\n` で結合（`\\n` の使用禁止）
- 監視名は「監視URL」へリンク（`<a href="URL">監視名</a>`）

#### 環境設定ファイル作成
```bash
# 本番用.env作成
cp .env.example .env.production

# 必要な環境変数設定
nano .env.production
```

```bash
# .env.production内容例
NODE_ENV=production
TELEGRAM_BOT_TOKEN=your_production_bot_token
TELEGRAM_CHAT_ID=your_production_chat_id
DATABASE_PATH=./data/production.db
LOG_LEVEL=info
MONITORING_INTERVAL=5
```

### 2. Docker本番デプロイ

#### 本番用docker-compose設定
```yaml
# docker-compose.prod.yml
version: '3.8'
services:
  sokubutsu:
    build:
      context: .
      dockerfile: Dockerfile.prod
    container_name: sokubutsu-mvp
    restart: always
    env_file:
      - .env.production
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    ports:
      - "3000:3000"
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "node", "healthcheck.js"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
```

#### 本番用Dockerfile
```dockerfile
# Dockerfile.prod
FROM node:18-alpine AS builder

WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

FROM node:18-alpine

RUN apk add --no-cache chromium
ENV PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=true
ENV PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser

WORKDIR /app
COPY --from=builder /app/node_modules ./node_modules
COPY . .

RUN npm run build

USER node
EXPOSE 3000

HEALTHCHECK --interval=30s --timeout=10s --retries=3 --start-period=40s \
  CMD node healthcheck.js

CMD ["npm", "run", "start:prod"]
```

#### デプロイ実行
```bash
# 本番ビルド・起動
docker-compose -f docker-compose.prod.yml up -d --build

# 起動確認
docker-compose -f docker-compose.prod.yml logs -f sokubutsu

# ヘルスチェック確認
docker-compose -f docker-compose.prod.yml ps
```

### 3. SSL証明書設定（HTTPS対応）

#### Let's Encrypt + nginx-proxy設定
```yaml
# docker-compose.ssl.yml
version: '3.8'
services:
  nginx-proxy:
    image: jwilder/nginx-proxy
    container_name: nginx-proxy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - /var/run/docker.sock:/tmp/docker.sock:ro
      - ./nginx/certs:/etc/nginx/certs
    restart: always

  letsencrypt:
    image: jrcs/letsencrypt-nginx-proxy-companion
    container_name: letsencrypt
    volumes_from:
      - nginx-proxy
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    restart: always

  sokubutsu:
    extends:
      file: docker-compose.prod.yml
      service: sokubutsu
    environment:
      - VIRTUAL_HOST=your-domain.com
      - LETSENCRYPT_HOST=your-domain.com
      - LETSENCRYPT_EMAIL=your-email@example.com
```

## 📊 監視・メンテナンス

### 1. ログ監視

#### ログ設定
```typescript
// src/logger.ts
import { createLogger } from 'vibelogger';

export const logger = createLogger({
  level: process.env.LOG_LEVEL || 'info',
  output: {
    console: true,
    file: {
      path: './logs/sokubutsu.log',
      rotation: 'daily',
      maxFiles: 30
    }
  },
  structured: true
});
```

#### ログ確認コマンド
```bash
# リアルタイムログ監視
docker-compose -f docker-compose.prod.yml logs -f sokubutsu

# 特定期間のログ確認
docker-compose -f docker-compose.prod.yml logs --since "2025-01-01" sokubutsu

# ログファイル直接確認
tail -f logs/sokubutsu.log

# エラーログのみフィルタ
docker-compose logs sokubutsu 2>&1 | grep -i error
```

### 2. システム監視

#### ヘルスチェック実装
```typescript
// healthcheck.js
const http = require('http');

const options = {
  host: 'localhost',
  port: 3000,
  path: '/health',
  timeout: 2000
};

const request = http.request(options, (res) => {
  if (res.statusCode === 200) {
    process.exit(0);
  } else {
    process.exit(1);
  }
});

request.on('error', () => process.exit(1));
request.end();
```

#### リソース使用量監視
```bash
# Dockerコンテナの統計情報
docker stats sokubutsu-mvp

# システムリソース確認
htop
free -h
df -h

# プロセス監視
ps aux | grep node
```

#### 監視スクリプト（cron設定）
```bash
# monitoring.sh
#!/bin/bash
CONTAINER_NAME="sokubutsu-mvp"
LOG_FILE="/home/user/logs/monitoring.log"

# コンテナ稼働確認
if ! docker ps | grep -q $CONTAINER_NAME; then
    echo "$(date): Container $CONTAINER_NAME is down. Restarting..." >> $LOG_FILE
    docker-compose -f /home/user/production/sokubutsu/docker-compose.prod.yml up -d
fi

# ディスク使用量確認（80%以上で警告）
DISK_USAGE=$(df / | awk 'NR==2 {print $(NF-1)}' | sed 's/%//')
if [ $DISK_USAGE -gt 80 ]; then
    echo "$(date): Disk usage is ${DISK_USAGE}%" >> $LOG_FILE
fi
```

```bash
# cronジョブ設定
crontab -e

# 5分ごとに監視スクリプト実行
*/5 * * * * /home/user/scripts/monitoring.sh

# 毎日午前2時にログローテーション
0 2 * * * find /home/user/logs -name "*.log" -mtime +30 -delete
```

### 3. バックアップ・復旧

#### データベースバックアップ
```bash
# backup.sh
#!/bin/bash
BACKUP_DIR="/home/user/backups"
DATE=$(date +%Y%m%d_%H%M%S)

# データディレクトリバックアップ
tar -czf $BACKUP_DIR/sokubutsu_data_$DATE.tar.gz -C /home/user/production/sokubutsu data/

# 古いバックアップ削除（30日以上）
find $BACKUP_DIR -name "sokubutsu_data_*.tar.gz" -mtime +30 -delete

echo "Backup completed: sokubutsu_data_$DATE.tar.gz"
```

#### 復旧手順
```bash
# サービス停止
docker-compose -f docker-compose.prod.yml down

# バックアップから復元
cd ~/production/sokubutsu
tar -xzf ~/backups/sokubutsu_data_YYYYMMDD_HHMMSS.tar.gz

# サービス再開
docker-compose -f docker-compose.prod.yml up -d

# 復旧確認
docker-compose -f docker-compose.prod.yml logs sokubutsu
```

## 🔧 トラブルシューティング

### 1. よくある問題と解決策

#### コンテナ起動しない
```bash
# ログ確認
docker-compose -f docker-compose.prod.yml logs sokubutsu

# イメージ再ビルド
docker-compose -f docker-compose.prod.yml down
docker-compose -f docker-compose.prod.yml build --no-cache
docker-compose -f docker-compose.prod.yml up -d
```

#### メモリ不足エラー
```bash
# メモリ使用量確認
free -h
docker stats

# Swap領域追加
sudo fallocate -l 2G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# 永続化
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
```

#### Puppeteerエラー（WSL2環境）
```bash
# Chrome依存関係インストール
sudo apt-get update
sudo apt-get install -y chromium-browser

# 環境変数設定
export PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=true
export PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser
```

#### ディスク容量不足
```bash
# 不要なDockerリソース削除
docker system prune -a

# 古いログファイル削除
find logs/ -name "*.log" -mtime +7 -delete

# データベースサイズ確認
ls -lah data/
```

### 2. パフォーマンス最適化

#### Docker設定最適化
```yaml
# docker-compose.prod.yml
services:
  sokubutsu:
    # リソース制限
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
    
    # ログドライバー設定
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
```

#### WSL2メモリ制限
```ini
# ~/.wslconfig
[wsl2]
memory=4GB
processors=4
swap=2GB
localhostForwarding=true
kernelCommandLine=cgroup_no_v1=memory systemd.unified_cgroup_hierarchy=1
```

## 🔄 継続的デプロイメント

### 1. GitHub Actions CI/CD設定

```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [ main ]

jobs:
  deploy:
    runs-on: self-hosted
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to production
      run: |
        docker-compose -f docker-compose.prod.yml down
        docker-compose -f docker-compose.prod.yml build --no-cache
        docker-compose -f docker-compose.prod.yml up -d
        
    - name: Health check
      run: |
        sleep 30
        curl -f http://localhost:3000/health || exit 1
        
    - name: Notify deployment
      run: |
        # Telegram通知等
```

### 2. ローリングデプロイメント
```bash
# zero-downtime-deploy.sh
#!/bin/bash
set -e

echo "Starting zero-downtime deployment..."

# 新しいコンテナ起動
docker-compose -f docker-compose.prod.yml up -d --scale sokubutsu=2 --no-recreate

# ヘルスチェック待機
sleep 30

# 古いコンテナ停止
docker-compose -f docker-compose.prod.yml up -d --scale sokubutsu=1 --no-recreate

echo "Deployment completed successfully"
```

## 📚 運用チェックリスト

### 日次チェック項目
- [ ] サービス稼働状況確認
- [ ] エラーログ確認
- [ ] ディスク使用量確認
- [ ] メモリ使用量確認

### 週次チェック項目
- [ ] データベースバックアップ確認
- [ ] ログファイルローテーション
- [ ] セキュリティアップデート確認
- [ ] パフォーマンス指標レビュー

### 月次チェック項目
- [ ] 古いバックアップファイル削除
- [ ] システム全体の健全性確認
- [ ] 容量計画見直し
- [ ] 災害復旧手順テスト

## 📞 緊急時対応手順

### サービス完全停止時
```bash
# 1. 状況確認
docker-compose -f docker-compose.prod.yml ps
docker-compose -f docker-compose.prod.yml logs --tail=100 sokubutsu

# 2. 強制再起動
docker-compose -f docker-compose.prod.yml down
docker-compose -f docker-compose.prod.yml up -d --force-recreate

# 3. ヘルスチェック
curl -f http://localhost:3000/health
```

### データ破損時
```bash
# 1. サービス停止
docker-compose -f docker-compose.prod.yml down

# 2. 最新バックアップから復旧
tar -xzf ~/backups/sokubutsu_data_latest.tar.gz

# 3. サービス再開
  docker-compose -f docker-compose.prod.yml up -d
```

---

## 🔧 ブランチ変更が実機に反映されないとき

- 原因になりやすいポイント:
  - 本番機で `main` 以外のブランチを追っていない（CIは `main` のみデプロイ）
  - Dockerイメージを再ビルドしていない（`build: .` だがキャッシュで古い `dist` を使用）
  - Telegramクライアント側のコマンドメニューがキャッシュされている（サーバ側の挙動は更新済みでもメニューに即時反映されない）

- 反映手順（ローカル/実機でカレントブランチを即反映）
  1) 対象ブランチにチェックアウト: `git checkout <branch>` → `git pull`
  2) 再ビルド＆再起動: `bash scripts/deploy-local.sh`
     - `--no-cache --pull` で確実に再ビルドします
  3) 動作確認
     - Webhook: `curl -s "https://api.telegram.org/bot$TELEGRAM_BOT_TOKEN/getWebhookInfo"`
     - 健康: `curl -I "${ADMIN_PUBLIC_URL%/}/health"`
     - Bot: `/help`, `/status`, `/check` の応答

- CIでのプレビューイメージ（ブランチごと）
  - GitHub Actionsが `ghcr.io/<repo>:branch-<ブランチ名>` を自動ビルド・プッシュ
  - 実機でテストする場合は compose を `image: ghcr.io/...:branch-<ブランチ名>` に切替 → `docker compose pull && up -d`

メモ: Telegramのコマンドメニューはクライアント側キャッシュが残る場合があります。メニューが即更新されない場合でも、直接 `/help` 等のコマンドは最新ロジックで動作します。

**更新日**: 2025年8月26日  
**バージョン**: 2.0（WSL2 + Docker本番運用対応）
